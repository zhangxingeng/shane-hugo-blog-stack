---
title: "我将来想写的一些博客主题"
draft: true
---

## 1. 设计可扩展的分布式深度学习基础设施：在6,000个服务器机架上的50,000块GB200 GPU上进行分布式深度学习，需要从硬件、网络到编排的每一层都进行全面考虑

## 2. 详细介绍PyTorch DDP及相关分布式训练工具的工作原理。例如：

| **并行方式**      | **方法**           | **用途**               |
|------------------|-------------------|------------------------|
| **数据并行**     | PyTorch DDP + SHARP | 标准多GPU训练           |
| **ZeRO Offload (DeepSpeed)** | 优化器状态卸载      | 降低GPU显存占用         |
| **张量并行**     | Megatron-LM        | 跨GPU切分张量           |
| **流水线并行**   | GPipe              | 跨GPU切分模型层         |

## 3. 
