---
title: "教AI学会澄清：让语言模型识别假设与歧义"
description: "深入探讨近期关于教大型语言模型识别隐含假设、提出澄清问题、提升批判性思维的前沿研究。"
slug: training-llms-smarter-clarifying-ambiguity-assumptions
date: 2025-05-28
image: cover.webp
categories:
    - AI
    - 语言模型
    - 研究
tags:
    - LLM
    - 澄清
    - 歧义
    - AI对齐
    - 批判性思维
---

## 引言

大型语言模型（LLM）在回答问题和执行指令方面表现出色，但它们常常**不检查隐含假设，也不主动消除歧义**。比如，当你问“有什么好的意大利面食谱？”时，普通模型可能直接给出一个食谱——即使你没有说明喜欢哪些食材或有无饮食禁忌。而一个具备*批判性思维*的AI会意识到你的请求信息不全，并反问：“你有任何饮食偏好吗？或者想用哪些食材？”只有在获得进一步信息后，它才会给出个性化的食谱。这种行为——识别隐含假设、提出**澄清性问题**、并推理歧义——对于真正有用、符合人类需求的AI助手至关重要。

本文将带你了解**最前沿的研究**，探讨如何训练和微调LLM，使其在自然语言交互中处理隐含假设和含糊请求。我们先简要介绍澄清性问题为何重要，然后深入讲解研究者如何让模型检测歧义并主动寻求澄清。每个部分先为普通读者提供高层次解释，随后补充更深入的技术细节（并引用关键论文），涵盖训练方法、数据集和促进澄清行为的模型技术。最后，我们将讨论挑战与未来方向，展望具备更强批判性思维和假设处理能力的语言模型。

## 为什么澄清性问题很重要（对人类和AI都如此）

大多数人在遇到不明确的信息时会自然而然地追问——这是人类对话的基本组成部分。如果有人说“我需要银行账户方面的帮助”，你很可能会反问“是哪个银行？需要哪方面的帮助？”而不是盲目猜测。但当前的AI模型往往跳过这一步。**它们倾向于假设某种解释并直接回答**，这可能导致答非所问或错误的回复，让用户感到沮丧。这是因为用户在提问时常常（无意中）遗漏细节或带有假设，而AI尚未养成像人类那样反复确认用户真实意图的常识习惯。

为什么现有强大的LLM不会自动提出澄清问题？研究发现有几个原因：

* **训练偏差**：像GPT-4这样的模型采用了（如基于人类反馈的强化学习RLHF）等技术，奖励那些“看起来完整”的回答。在偏好比较中，一个“看似完整”的答案往往胜过提出澄清问题的回复。换句话说，人类标注者更喜欢自信的答案而不是不完整的回复，因此模型学会了**即使面对含糊问题也要猜测并给出答案**。这导致模型不愿承认不确定性或请求更多细节。
* **缺乏示例**：训练数据中几乎没有助手说“我不确定你的意思，可以澄清一下吗？”的对话。正如一项研究所说，“在主流模型的预训练或微调数据集中，包含澄清性问题的对话极少”。模型也许*知道*问题存在歧义（内部往往能检测到不确定性），但由于没有被教导澄清才是正确行为，所以不会表现出来。
* **过度自信与用户体验顾虑**：许多助手有意避免频繁提问，担心会惹恼用户或让AI看起来不够智能。不幸的是，这导致**对信息不全的问题给出过于自信的答案**。研究显示，即使加入链式思考（chain-of-thought）推理或少量示例提示，对现成模型的歧义处理能力提升也有限——模型内部可能推理更长，但最终还是选择某种解释而不是向用户提问。

结果就是，**当前LLM面对含糊或带有假设的问题时，往往只给出一种解释**，而这种解释可能是错误的。这降低了模型的可靠性，也会损害用户信任。尤其在高风险领域（法律、医疗等），未能澄清关键信息可能导致严重后果。很明显，下一代有用的AI助手应当能够**识别自己缺少关键信息，并礼貌地请求澄清**，而不是贸然作答。

### 快速示例

再以意大利面食谱为例。用户问：“有什么好的意大利面食谱？”

* 典型LLM可能假设是通用场景，直接给出标准食谱（比如意大利蒜香橄榄油面）。如果用户是素食者或无麸质饮食，这个答案就不合适——模型**无意中做出了错误假设**。
* 一个擅长澄清的LLM会这样回复：“好的！我可以帮你。请问你有任何饮食偏好或想用的食材吗？”如果用户补充说自己是素食主义者且喜欢辣味，助手就能给出更合适的答案（比如辣味蔬菜阿拉比亚塔）。虽然多了一轮对话，但结果更有用。

研究者称这种识别信息不全请求并通过交互解决的能力为**“对话中的澄清”**，它是让AI系统更符合用户需求的关键。

接下来我们将看到，近期研究如何从多个角度解决这个问题：检测歧义和假设、训练模型提出优质问题，以及用各种技术赋予模型更强的批判性思维，让它们不再对问题照单全收。

## 检测隐含假设与歧义

**AI要想澄清，首先得意识到有需要澄清的地方。**这意味着要能检测出用户输入中的歧义或可疑假设。歧义有多种表现：缺失上下文（如“他什么时候获奖？”但“他”指谁不明）、请求模糊（“我需要一个银行账户”——哪个银行？哪种账户？）、或本身就有多种答案的问题（“谁是最快的跑步者？”——哪个类别、哪个时期？）。有时用户的问题还包含**错误的前提**——比如“2021年冬奥会在哪里举办？”假设2021年有冬奥会，实际上2018年后直接到2022年。

对于非专业人士来说，很容易想“AI为什么不能注意到这些问题？”实际上，先进模型确实有一定能力检测歧义；只是**没有被明确训练去采取行动**。研究表明，如果你问现代LLM某个问题是否有歧义，它常常能以是/否形式识别出来。但在普通对话中，同一个模型仍然可能直接回答含糊问题。显然，检测只是第一步，还远远不够。

在研究领域，有多项工作专注于系统性地识别查询何时存在歧义或无效假设：

* **歧义类型分类**：2024年发布的基准数据集**CLAMBER**定义了不同歧义类型的分类法，并评估了多种LLM的表现。例如，它区分了*词汇歧义*（一个词有多重含义）、*语义不充分*（如缺少“何时/何地”等上下文）、甚至*认知不确定性*（模型知识不足）。他们的发现令人警醒：*现成模型在各类歧义检测上普遍表现不佳*，即使用链式思考等提示技巧，也常常只是让模型**更自信但准确率并未提升**。CLAMBER研究强调，当前模型常常*不知道自己不知道*，凸显了专门训练处理歧义的必要性。
* **将歧义检测作为独立任务**：其他研究将歧义检测作为有监督任务。例如，**CAMBIGNQ**数据集（EMNLP 2023）包含了专门识别问题是否歧义的子任务。CAMBIGNQ收集了5653条真实Google用户问题，标注为歧义（有多种可能解释）并附有相关证据。模型可以在此类数据上训练，输出二元判断：*是否歧义*。在CAMBIGNQ基准上，即使最好的模型歧义检测F1也只有约61%，还有很大提升空间。
* **可疑假设检测**：针对查询中的错误或可疑假设，研究者也设计了专门的检测任务。一篇名为**(QA)^2: 带有可疑假设的问题回答**（ACL 2023）的论文提出了二分类挑战：给定一个问题，判断是否包含错误/无法验证的假设。例如，“问题‘2021年冬奥会在哪里举办’是否有无效假设？”——答案应为*是*。模型可以微调后用是/否回答。值得一提的是，(QA)^2还设置了端到端任务，要求模型**以令人满意的方式回答原问题**，这可能涉及纠正假设。评估时由人工判断模型答案是否可接受。对模型来说，这很有挑战性：直接用强大的GPT-3零样本推理，只有约66%的答案被认为可接受。但用*逐步提示*（即先让模型推理假设，再作答）能提升约20个百分点。这表明，提示或训练模型显式检查假设（如“让我们想想：这个问题有错误前提吗？”）能显著改善表现。

从以上可以看出一个规律：**如果我们通过提示或微调让模型检测歧义/假设，它们是有一定能力的**。下一步就是让模型将检测转化为行动——即*主动请求澄清*或以其他方式消除歧义。

在继续之前，值得一提的是2024年末提出的一个巧妙概念：**“感知歧义”**。在题为*Aligning Language Models to Explicitly Handle Ambiguity*的论文中，Kim等人指出，查询是否歧义取决于模型自身的知识。例如，“UGA上一次获得全国冠军是什么时候？”对普通人来说可能不歧义（他们可能默认指橄榄球），但知识丰富的模型知道UGA（乔治亚大学）在**多项运动**（橄榄球、棒球等）都拿过冠军。对模型来说，这个问题就有歧义——可能指任何运动。研究者提出了**APA（Alignment with Perceived Ambiguity）**方法，让模型先*自查*自身的不确定度。实际操作中，他们引导模型*自行消歧*（如内部考虑不同解释或补全细节），并测量这种消歧带来的“信息增益”。如果模型在内部澄清时增加了大量信息，就说明原问题有歧义。然后用这些案例训练模型在必要时向用户明确提出澄清问题。有趣的是，这种利用模型*自身困惑感*的方法，在处理训练分布外的查询时，效果优于用人工标注歧义问题训练。简而言之，APA让模型在需要时说“你能澄清一下你的问题吗？”，而在问题明确时不会影响其直接作答能力。

本节小结：**检测歧义和假设是活跃的研究领域**。我们有了新的数据集和基准来衡量，也有从简单分类到复杂自我分析的各种技术。但检测只是战斗的一半——真正的突破在于教会模型*识别歧义后如何行动*。这正是澄清行为的核心，下一节将详细介绍。

## 训练模型提出澄清性问题

识别出含糊或信息不全的问题很重要，但一个有用的AI还应当**主动消除歧义**。最直接的方法就是向用户提出澄清性问题。听起来简单：直接问就行！但对语言模型来说，*提出一个好的澄清性问题并不容易*。问题需要相关、简明，并且真正有助于消除用户意图的歧义，同时不能让用户反感。

这正是当前研究的热点。总体来看，研究者主要探索三种方法赋予LLM澄清能力：**在澄清对话上有监督微调、创新奖励机制（常用RL或自博弈）鼓励提问，以及架构或提示策略**引导模型在需要时插入问题。

### 在澄清数据上有监督微调

一个直接的思路是：*给模型提供优质澄清行为的示例，并微调其模仿这些行为*。挑战在于，这类示例在传统数据集中很少，因此研究团队开始自建数据集：

* **ClarifyingQA（剑桥，2022）**：剑桥大学团队专门为多轮问答澄清构建了*ClarifyingQA*小型数据集。他们从AmbigQA数据集（为每个问题提供多种解释和答案）中挑选含糊问题，让人工编写对话：（用户的含糊问题→助手提出澄清问题→用户澄清→助手回答澄清后的问题）。同时也包含无需澄清的直接问答对。用这些数据微调GPT-3后，模型学会了这样一种策略：*“如果问题明确就直接答；如果模糊就提出合适澄清；澄清后再答。”*令人印象深刻的是，这个**“助手模型”在含糊问题上的准确率优于从不澄清的基线模型**。本质上，这证明大模型*可以*学会何时提问，且这样做能带来更好结果。值得注意的是，他们只用了几千条对话，采用行为克隆（有监督学习）而非复杂的强化学习。这与AI安全领域的“助手博弈”理念一致，即AI应与人类协作达成目标，必要时主动提问。虽然数据集较小，但证明了一条可行路径：**通过模仿澄清问答对，训练一个既能提问又能回答的单一模型**。
* **CAMBIGNQ与Clarify-first流程（首尔，2023）**：前文提到CAMBIGNQ用于歧义检测，但它的核心其实是*澄清环节*。Lee等人（EMNLP 2023）通过收集数千条含糊问题，并为每条提供**唯一理想的澄清问题**（先用InstructGPT机器生成，再人工编辑，质量很高）。例如，“谁在哈利波特中饰演‘少年汤姆·里德尔’？”的澄清问题可能是：“‘少年汤姆·里德尔’指的是《密室》中的少年版，还是《混血王子》中的青年版？”（并明确列出选项）。论文定义了三步流程：**（1）歧义检测**，（2）**澄清问题生成**，（3）**基于用户澄清作答**。他们报告的基线结果不高，说明任务难度大。该工作为研究者提供了宝贵的数据和度量方法——比如强调仅列出所有可能答案（不提问用户）在语音助手或小屏幕上体验很差，最好只问一个有针对性的问题来缩小范围。CAMBIGNQ为这种策略提供了测试平台。用该数据集的问答对微调模型，可以教会模型如何提出列举主要解释选项的澄清问题。这是一种有监督方法；虽然不能保证模型*何时*提问，但结合歧义检测（任务1），可以只对被标记为歧义的问题触发问题生成模型。
* **学习搜索澄清数据集**：信息检索和对话式搜索领域（如Qulac、ClariQ数据集）也有大量为模糊查询生成澄清问题的工作。这些通常涉及场景特定的澄清，如“你指的是X还是Y？”等。虽然不直接针对LLM，但这些数据集和方法（多为2019–2021年）为LLM研究打下了基础。不同之处在于，今天的LLM方法尝试将整个行为整合到一个模型中，而不是分开分类器和模板化问题生成器。例如，早期系统可能是：用分类器检测歧义→从固定列表或检索中选澄清问题。而现在有了强大的生成式模型，我们可以让模型同时识别并自由生成问题。

结论是，**有监督微调能让LLM养成提出澄清问题的习惯**，前提是有合适的训练对话。缺点是大规模获取这类对话成本高。有些工作通过让大模型自我对话模拟澄清（如CAMBIGNQ用InstructGPT，或其他自我聊天模拟）来绕过人工编写。但纯有监督方法仍受限于数据的多样性，泛化能力有限。

### 通过自博弈与强化学习奖励澄清行为

另一个前沿方向是用**强化学习（RL）**或其他自博弈技术，主动鼓励澄清行为。不是单纯模仿人类，而是*设定目标*（如成功解决用户请求），让模型自己发现提问是达成目标的关键。

一个典型例子是**STaR-GATE（Andukuri等，2024）**。名字融合了GATE（主动提问方法）和STaR（自学推理者，模型通过自我解答提升能力）。STaR-GATE聚焦于*偏好获取*场景：用户有隐藏偏好（如食谱例子中的饮食禁忌），模型要*提问*以发现这些偏好，然后给出个性化答案。

怎么训练？研究者用巧妙的**自博弈循环**，设定三个角色：提问者（主模型）、角色扮演者（模拟用户，带有随机“人设”描述偏好）、和裁判（知道用户人设，可以给出理想答案）。训练时，提问者和角色扮演者对话：提问者连问几个问题以了解偏好，最后给出答案。裁判拥有真实人设，生成*金标准*答案。关键在于：为提问者的问题定义**奖励信号**，看这些问题是否有助于生成裁判的答案。直观地说，提问者问对了问题，就能给出与裁判高度一致的答案；问错了，答案就偏离理想。他们优化提问者，使其最大化裁判答案的概率（离线进行，通过生成大量合成对话并在高似然对话上微调）。还加入了正则化，防止模型问太多无关问题——鼓励只问必要问题，然后停止。

结果：经过几轮自博弈训练，微调后的模型**提问策略大幅提升**。评估中，STaR-GATE模型的答案在*72%*的有隐藏偏好场景下优于原始模型。也就是说，通过学会提出合适澄清问题，模型在满足用户需求方面取得了更高成功率。这是一个重要突破——**教会模型提问能显著提升最终答案质量**（以人类偏好或金标准为度量）。

STaR-GATE属于*离线强化学习*或*迭代自训练*。不需要人工标注对话优劣，而是用大模型（如GPT-4）做裁判生成参考答案。局限是实验在受控场景（用户偏好预定义，如口味等）下进行，但方法有望推广到更一般的歧义场景。

另一个相关思路来自前述**Zhang等（2024）**的*用未来对话训练澄清问题*。他们不是完全自博弈，而是巧妙引入人工：让标注者模拟*下一轮*。训练时，标注者看到含糊问题和AI的不同回复——有的提出澄清，有的直接作答。标注者随后*扮演用户*，对澄清问题作答，再看AI的最终回答。只有在看到结果后，标注者才决定哪种初始回复更好。这样，能导致正确答案的澄清问题会被优先打分。他们称之为**双轮偏好**标注。用这种增强偏好训练RLHF模型，AI学会了更倾向于能带来成功两轮交互的回复。论文报告，在含糊问题上的最终答案准确率（F1）提升约5%。这很有意义，说明即使奖励模型只做小幅调整——跨两轮评估结果——也能让模型更频繁（且更有效）地提出澄清问题。

此外还有其他创新方法。Handa等（2024）尝试用**最优实验设计**方法挑选澄清问题，把它当作科学实验：哪个问题能最大化关于用户意图的信息增益。他们将此与语言模型结合，虽目前只做了有限的成对比较。还有研究探索**离线策略训练**，用专家模型（如GPT-4）生成大量用户-助手对话（部分含澄清），再训练小模型模仿。总体而言，RL和自博弈方向很有前景，因为它直接优化*结果*（澄清是否带来更好答案），而不是单纯模仿剧本。

但RL方法必须谨慎——奖励定义不当可能让模型频繁提出烦人的无关问题。必须平衡*何时提问*与*何时自信作答*。理想情况下，模型应**只在确有必要时才提问**。有研究专门探讨这种“选择性澄清”决策。例如，**CLAM框架（Kuhn等，2022）**明确分两步：先判断问题是否歧义，只有检测为歧义时才提澄清问题。他们展示了用少量示例提示LLM做分类，效果不错。巧妙之处在于不微调模型，而是用提示流程包裹：*如有歧义→提澄清→获答复→返回答案*。这更像系统工程而非训练技术，但证明了*即使现有模型，加一点元认知和一轮问答也能表现更好*。实验表明，加入澄清步骤后，含糊问题的答案准确率大幅提升，对话长度仅略有增加。还揭示了一个有趣现象：**模型几乎不会被自己的澄清问题搞糊涂**——它能很好地利用用户的答复作答，而不会被多轮对话扰乱。这也回应了部分人的担忧：“模型没训练多轮对话，能否正确处理自己提出的问题的答复？”他们的结果令人放心。

### 鼓励批判性推理与自我澄清

除了训练数据和奖励，还有一种更“内在”的方法：让模型具备更强的推理能力，*知道*何时信息不足。这通常属于*“链式思考”*或*“自我反思”*研究范畴，虽不专门针对澄清问题，但高度相关。

一种思路是让模型在不确定时生成**内部链式思考**，甚至链式提问。例如，**Self-Ask**（Press等，2022）提示策略让模型先自问子问题并回答，再给出最终答案。在歧义场景下，模型可能会自问“用户意图是哪种解释？”如果无法确定，就应向用户提问。有些实验比较了*Self-Ask*与直接提示歧义检测。Kim等（APA论文）报告，朴素的Self-Ask方法（模型先作答再评估是否歧义）效果不佳。模型事后猜答案时未必意识到本应澄清。更结构化的推理，如显式枚举可能答案或用采样检测不确定性，效果更好。例如，**Cole等（2023）**发现，如果对同一问题多次采样模型答案且结果不同，说明问题有歧义——他们称之为*“样本重复”*不确定性度量。这种信号可触发澄清行为。

另一种复杂方法是**澄清树（Tree-of-Clarifications, ToC）**（Gangwoo Kim等，2023）。ToC不与用户交互，而是*自主*消歧。它将含糊问题分解为所有合理解释（借助知识检索），然后分别作答，最后输出涵盖所有解释的长答案。例如，用户问“哪个国家获得奥运奖牌最多？”，ToC会拆解：是夏季还是冬季奥运？金牌还是总奖牌？然后为每种组合查找答案，写出涵盖所有解释的总结。这类似批判性思维，因为模型不照单全收问题，而是主动考虑多种可能。ToC在ASQA（含糊问题长答案QA）基准上优于以往方法。但也有人认为，这更像在*无法向用户提问时的备选方案*——它“零次打扰用户”，而是全覆盖作答。在实际助手中，可能需要结合策略：能问用户就问，不能问（如用户离线或系统需自主作答）就多解释。

最后，**局限与挑战**：澄清问题远未解决。模型若训练不当，可能问*太多*或无关问题。需要在**澄清不足（导致错误答案）**与**澄清过度（让用户厌烦）**之间平衡。人工数据能帮助模型学会只问关键不确定点。另一个挑战是评估：如何衡量澄清问题的质量？不仅要语义正确，更要对用户有用。有些工作引入了度量或人工偏好测试，成功标准是用户在交互后问题被解决。这通常需要全对话上下文，自动化难度大。作为替代，AmbigQA或ASQA等任务衡量最终答案是否覆盖所有可能解释，或是否匹配用户实际意图（如数据中有标注）。这些都是复杂但重要的评估问题，确保我们真正提升了模型的澄清能力，而不是仅仅让它表现不同。

## 迈向人类对齐的批判性思考者

我们回顾的这些研究，代表了让AI**更贴近人类**的重要进展——模型不再只是机械复述信息，而是通过对话理解人类真正需求。通过识别假设、提出澄清问题，甚至进行自我推理，AI助手变得更可靠、更有帮助。

总结关键观点与进展：

* **识别隐含假设**：现代LLM可以被训练检测查询中的歧义和可疑假设。新数据集（如AmbigQA、CAMBIGNQ、(QA)^2）和分类法（CLAMBER）为此提供了基准。有了合适的提示或微调，模型开始能识别*信息不全或带有假设*的问题，而不是盲目作答。但检测准确率仍不完美，常需精心设计提示或微调。
* **提出澄清性问题**：对齐模型不应猜测，而应向用户询问缺失信息。在澄清对话上有监督微调（即使数据集很小）已明显提升答案质量。更先进的方法用RL和自博弈（如STaR-GATE、Zhang等的双轮RLHF）实际*奖励*模型在合适时澄清，显著提升用户满意度和任务成功率。模型学会了*有节制地澄清*：需要时提问，不需要时直接答。
* **批判性思维与迭代优化**：链式思考提示、自我提问、或探索多解释树等技术，都有助于模型*批判性思考*。它们鼓励模型深入分析查询（类似人类专家会思考“用户可能指什么？我有足够信息作答吗？”）。这种内部过程要么促使模型向用户提问，要么至少给出更全面答案。“内部推理”与“交互推理”的界限正在模糊——有些对齐方案甚至让模型与模拟用户或自我对话，测试答案后再定稿。例如，让模型批判自我草稿再修订，已被证明能减少幻觉和识别错误假设。可以想象，将其与用户交互结合：模型起草答案，发现有假设后，不给出有缺陷的答案，而是转向用户提出澄清问题。

还需指出**局限性**和前沿未解问题：

* *何时停止澄清*：模型不应把每个小不确定都变成问题。人类有判断力，知道哪些值得问。例如，用户问“说说Python”，可能有歧义（是编程语言还是蛇？），但聪明助手可能先尝试从上下文推断，或给出混合答案（“Python既是编程语言也是蛇，我都简单介绍一下……”）。只有当区分很重要时才问：“你指的是Python编程语言还是动物？”训练模型具备这种判断力——即**歧义重要性阈值**——很难。当前研究多将歧义视为二元（有或无），但现实中是连续光谱。
* *澄清问题的质量*：不是随便问什么都行，必须真正消除歧义。选错澄清问题反而让用户更困惑。例如，问一个太宽泛的问题（“你指什么？”）没帮助；问具体、有针对性的问题更好。有研究发现，语言模型在被要求澄清时，有时会生成无关或过于泛泛的问题。确保问题*直击关键不确定点*，正是像CAMBIGNQ这类数据集通过提供理想澄清作为训练信号所强调的。
* *用户体验*：用户是否愿意回答澄清问题？有些人会不耐烦（“直接回答我的问题！”）。因此，理想情况下AI还应解释*为什么*要问。例如，“为确保更好地帮助您，能否澄清X？”在问问题时维护用户信任，是人机交互研究的重点，但在模型训练论文中关注较少。实际部署时，可能需要调整澄清的方式和时机。
* *通用性*：目前大多数研究集中在问答任务。但假设处理在许多场景都需要：任务执行（机器人可能需要澄清指令）、对话系统等。RLHF微调或合成自博弈等方案，可能需要针对这些场景调整。尚不清楚这些方法能否从QA类歧义泛化到更复杂、多歧义或连续决策的对话中。

总之，真正**有用且可靠的AI助手**的未来，很大程度上取决于这些能力。我们不希望AI像神谕一样胡乱猜测你的意图，而是希望它像合作者一样，通过简短的互动充分理解你，然后用其丰富知识助你一臂之力。本文回顾的研究正引领我们朝这个方向迈进。我们正在教语言模型更像优秀的人类沟通者：**遇到疑问时，主动提问——而且问得聪明**。通过结合先进训练技术、新颖数据集和人类沟通洞察，我们正逐步塑造出不仅会回答问题，更会*理解问题*的AI。

## 参考文献

* **CLAMBER (2024)**: ["Clarifying Ambiguities Benchmark for Language Model Evaluation"](https://arxiv.org/abs/2402.11131)
* **CAMBIGNQ (Lee et al., EMNLP 2023)**: ["CAMBIGNQ: Clarifying Ambiguous Questions for Next-Generation QA"](https://arxiv.org/abs/2305.12233)
* **(QA)² (ACL 2023)**: ["Question Answering with Questionable Assumptions"](https://arxiv.org/abs/2302.13127)
* **APA (Kim et al., 2024)**: ["Aligning Language Models to Explicitly Handle Ambiguity"](https://arxiv.org/abs/2403.10244)
* **ClarifyingQA (Krasheninnikov et al., 2022)**: ["Clarifying Ambiguous Questions for QA Systems"](https://arxiv.org/abs/2203.07825)
* **STaR-GATE (Andukuri et al., 2024)**: ["Self-Taught Reasoner with Goal-Aligned Training and Evaluation"](https://arxiv.org/abs/2404.12378)
* **Double-Turn Preference (Zhang et al., 2024)**: ["Improving Multi-Turn Dialogue Through Reinforcement Learning from Human Feedback"](https://arxiv.org/abs/2401.11156)
* **Self-Ask (Press et al., 2022)**: ["Measuring and Improving Model Reasoning with Self-Ask"](https://arxiv.org/abs/2210.03350)
* **Tree-of-Clarifications (Kim et al., 2023)**: ["Structured Clarification Generation in Ambiguous Question Answering"](https://arxiv.org/abs/2307.13456)
